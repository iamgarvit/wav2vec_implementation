{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4632e410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:04.887587Z",
     "iopub.status.busy": "2025-12-07T17:58:04.887355Z",
     "iopub.status.idle": "2025-12-07T17:58:08.743129Z",
     "shell.execute_reply": "2025-12-07T17:58:08.742520Z",
     "shell.execute_reply.started": "2025-12-07T17:58:04.887566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import soundfile as sf\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ddd701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.744983Z",
     "iopub.status.busy": "2025-12-07T17:58:08.744680Z",
     "iopub.status.idle": "2025-12-07T17:58:08.750611Z",
     "shell.execute_reply": "2025-12-07T17:58:08.749929Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.744965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_masker_helper(B, T, mask_prob=0.065, mask_length=10, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    if T <= 0 or B <= 0:\n",
    "        raise ValueError(\"T and B must be positive integers.\")\n",
    "    \n",
    "    mask = torch.zeros(B, T, dtype=torch.bool, device=device)\n",
    "    mask_length = max(1, min(mask_length, T))\n",
    "\n",
    "    target_num_mask = int(mask_prob * T)\n",
    "    num_spans = max(1, target_num_mask // mask_length)\n",
    "\n",
    "    for b in range(B):\n",
    "        starts = torch.randint(0, T, (num_spans,), device=device)\n",
    "        \n",
    "        for start in starts:\n",
    "            end = min(start + mask_length, T)\n",
    "            mask[b, start:end] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0e95d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.751597Z",
     "iopub.status.busy": "2025-12-07T17:58:08.751440Z",
     "iopub.status.idle": "2025-12-07T17:58:08.772129Z",
     "shell.execute_reply": "2025-12-07T17:58:08.771418Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.751584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LibriSpeechDataset(Dataset):\n",
    "    def __init__(self, directory_path):\n",
    "        self.directory = pathlib.Path(directory_path)\n",
    "        self.samples = []\n",
    "\n",
    "        for speaker in os.listdir(directory_path):\n",
    "            speaker_path = os.path.join(directory_path, speaker)\n",
    "            if not os.path.isdir(speaker_path):\n",
    "                continue\n",
    "\n",
    "            for book in os.listdir(speaker_path):\n",
    "                chapter_path = os.path.join(speaker_path, book)\n",
    "                if not os.path.isdir(chapter_path):\n",
    "                    continue\n",
    "\n",
    "                files = os.listdir(chapter_path)\n",
    "                transcript_file = glob.glob(os.path.join(chapter_path, \"*.trans.txt\"))\n",
    "                if not transcript_file:\n",
    "                    transcript_file = glob.glob(os.path.join(chapter_path, \"*.txt\"))\n",
    "                if not transcript_file:\n",
    "                    continue\n",
    "\n",
    "                transcript_file = transcript_file[0]\n",
    "                transcript_dict = {}\n",
    "                with open(transcript_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        id, text = line.strip().split(' ', 1)\n",
    "                        transcript_dict[id] = text\n",
    "\n",
    "                for file in files:\n",
    "                    if file.lower().endswith('.flac'):\n",
    "                        base_name = file.replace('.flac', '')\n",
    "                        audio_path = os.path.join(chapter_path, file)\n",
    "\n",
    "                        transcript = transcript_dict.get(base_name)\n",
    "                        if transcript is None:\n",
    "                            continue\n",
    "\n",
    "                        self.samples.append({\n",
    "                            'id': base_name,\n",
    "                            'audio_path': audio_path,\n",
    "                            'transcript': transcript\n",
    "                        })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        data, sr = sf.read(sample['audio_path'], dtype='float32')\n",
    "\n",
    "        if data.ndim == 1:\n",
    "            waveform = torch.from_numpy(data).unsqueeze(0)\n",
    "        else:\n",
    "            waveform = torch.from_numpy(data.T)\n",
    "        \n",
    "        w = waveform.squeeze(0)\n",
    "        mean = w.mean()\n",
    "        std = w.std() if w.std() > 1e-6 else 1.0\n",
    "        w = (w - mean) / std\n",
    "\n",
    "        waveform = w.unsqueeze(0)\n",
    "        return {\n",
    "            'id': sample['id'],\n",
    "            'waveform': waveform,\n",
    "            'sr': sr,\n",
    "            'transcript': sample['transcript'],\n",
    "            'path': sample['audio_path']\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    waveforms = [item['waveform'].squeeze(0) for item in batch]  \n",
    "    lengths = torch.tensor([len(waveform) for waveform in waveforms], dtype=torch.long)\n",
    "    \n",
    "    max_len = lengths.max().item()\n",
    "    padded_audios = torch.zeros(len(waveforms), max_len)\n",
    "    \n",
    "    for i, waveform in enumerate(waveforms):\n",
    "        padded_audios[i, :len(waveform)] = waveform\n",
    "    \n",
    "    return {\n",
    "        'audio': padded_audios,\n",
    "        'lengths': lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf3d0b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.774076Z",
     "iopub.status.busy": "2025-12-07T17:58:08.772960Z",
     "iopub.status.idle": "2025-12-07T17:58:08.790904Z",
     "shell.execute_reply": "2025-12-07T17:58:08.790364Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.774056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 seed=None, \n",
    "                 in_channels=1, \n",
    "                 conv_channels=[512, 512, 512, 512, 512, 512, 512],\n",
    "                 kernel_sizes=[10, 3, 3, 3, 3, 2, 2],\n",
    "                 strides=[5, 2, 2, 2, 2, 2, 2],\n",
    "                 dropout=0.1,\n",
    "                 sample_rate=16000):\n",
    "        super().__init__()\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_channels = list(conv_channels)\n",
    "        self.kernel_sizes = list(kernel_sizes)\n",
    "        self.strides = list(strides)\n",
    "        self.dropout_prob = dropout\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        assert(len(self.conv_channels) == len(self.kernel_sizes) == len(self.strides))\n",
    "        self.paddings = [k // 2 for k in self.kernel_sizes]\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "        prev_channels = self.in_channels\n",
    "\n",
    "        for out_c, k, s, p in zip(self.conv_channels, self.kernel_sizes, self.strides, self.paddings):\n",
    "            conv = nn.Conv1d(in_channels=prev_channels, out_channels=out_c, kernel_size=k, stride=s, padding=p, bias=False)\n",
    "            self.conv_layers.append(conv)\n",
    "            self.norm_layers.append(nn.LayerNorm(out_c, eps=1e-5))\n",
    "            self.activations.append(nn.GELU())\n",
    "            self.dropouts.append(nn.Dropout(p=self.dropout_prob))\n",
    "            prev_channels = out_c\n",
    "        self._init_weights()    \n",
    "\n",
    "    def _init_weights(self):\n",
    "        for conv in self.conv_layers:\n",
    "            nn.init.kaiming_normal_(conv.weight, nonlinearity='relu')\n",
    "\n",
    "        for ln in self.norm_layers:\n",
    "            nn.init.ones_(ln.weight)\n",
    "            nn.init.zeros_(ln.bias)\n",
    "\n",
    "    def compute_output_lengths(self, input_lengths):\n",
    "        if isinstance(input_lengths, int):\n",
    "            L = torch.tensor([input_lengths], dtype=torch.long)\n",
    "        elif isinstance(input_lengths, (list, tuple)):\n",
    "            L = torch.tensor(list(input_lengths), dtype=torch.long)\n",
    "        elif torch.is_tensor(input_lengths):\n",
    "            L = input_lengths.to(torch.long).clone()\n",
    "            if L.dim() == 0:\n",
    "                L = L.unsqueeze(0)\n",
    "        else:\n",
    "            raise ValueError(\"input_lengths must be an int, list, tuple, or torch.Tensor\")\n",
    "        \n",
    "        for k, s, p in zip(self.kernel_sizes, self.strides, self.paddings):\n",
    "            L = (L + 2 * int(p) - int(k)) // int(s) + 1\n",
    "            L = torch.clamp(L, min=1)\n",
    "        return L.long()\n",
    "    \n",
    "    def forward(self, x, input_lengths=None):\n",
    "        \"\"\"\n",
    "        input_lengths are in number of samples\n",
    "        output is in frames\n",
    "        padding mask returns true for padded positions\n",
    "        \"\"\"\n",
    "        if (x.dim() == 2):\n",
    "            x = x.unsqueeze(1)\n",
    "        elif (x.dim() == 3):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must be (B, T) or (B, 1, T)\")\n",
    "        \n",
    "        for i, (conv, norm, activ, drop) in enumerate(zip(self.conv_layers, self.norm_layers, self.activations, self.dropouts)):\n",
    "            x = conv(x)                          # (B, C_out, T_out)\n",
    "            x = x.transpose(1,2)                 # (B, T_out, C_out)\n",
    "            x = norm(x)                          \n",
    "            x = activ(x)\n",
    "            x = drop(x)\n",
    "            x = x.transpose(1, 2)                # (B, C_out, T_out)\n",
    "\n",
    "        features = x.transpose(1, 2).contiguous()        # (B, T_encoder, C)\n",
    "\n",
    "        if input_lengths is None:\n",
    "            return features\n",
    "        \n",
    "        output_lengths = self.compute_output_lengths(input_lengths)\n",
    "        output_lengths = output_lengths.to(features.device)\n",
    "\n",
    "        B, T_e, _ = features.shape\n",
    "        if (output_lengths > T_e).any():\n",
    "            output_lengths = torch.clamp(output_lengths, max=T_e)\n",
    "\n",
    "        pos = torch.arange(T_e, device=features.device).unsqueeze(0).expand(B, T_e)\n",
    "        padded_mask = pos >= output_lengths.unsqueeze(1)\n",
    "\n",
    "        assert padded_mask.shape == (B, T_e)\n",
    "        assert output_lengths.dtype == torch.long\n",
    "\n",
    "        return features, output_lengths, padded_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d3464a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.791741Z",
     "iopub.status.busy": "2025-12-07T17:58:08.791534Z",
     "iopub.status.idle": "2025-12-07T17:58:08.808580Z",
     "shell.execute_reply": "2025-12-07T17:58:08.807933Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.791716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Masker(nn.Module):\n",
    "    def __init__(self, embed_dim, mask_prob=0.065, mask_length=10):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.mask_prob = float(mask_prob)\n",
    "        self.mask_length = int(mask_length)\n",
    "        self.mask_emb = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "    def get_mask(self, B, T, device=None):\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "        return get_masker_helper(B, T, self.mask_prob, self.mask_length, device)\n",
    "    \n",
    "    def apply_mask(self, Z, mask):\n",
    "        assert Z.dim() == 3, \"Expected Z shape: [B, T, C]\"\n",
    "        B, T, C = Z.shape\n",
    "        if mask.shape != (B, T):\n",
    "            raise ValueError(f\"Mask shape {mask.shape} does not match Z shape {Z.shape}\")\n",
    "\n",
    "        if not mask.any():\n",
    "            b_idx = torch.empty(0, dtype=torch.long, device=Z.device)\n",
    "            t_idx = torch.empty(0, dtype=torch.long, device=Z.device)\n",
    "            return Z, b_idx, t_idx, mask\n",
    "\n",
    "        Z_masked = Z.clone()\n",
    "        Z_masked[mask] = self.mask_emb\n",
    "        b_idx, t_idx = torch.where(mask)\n",
    "        return Z_masked, b_idx, t_idx, mask\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        assert Z.dim() == 3, \"Expected Z shape: [B, T, C]\"\n",
    "        B, T, C = Z.shape\n",
    "        device = Z.device\n",
    "\n",
    "        if not self.training:\n",
    "            b_idx = torch.empty(0, dtype=torch.long, device=Z.device)\n",
    "            t_idx = torch.empty(0, dtype=torch.long, device=Z.device)\n",
    "            mask = torch.zeros(B, T, dtype=torch.bool, device=Z.device)\n",
    "            return Z, b_idx, t_idx, mask\n",
    "\n",
    "        mask = self.get_mask(B, T, device)\n",
    "        return self.apply_mask(Z, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db6a6c0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.809481Z",
     "iopub.status.busy": "2025-12-07T17:58:08.809315Z",
     "iopub.status.idle": "2025-12-07T17:58:08.824020Z",
     "shell.execute_reply": "2025-12-07T17:58:08.823427Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.809467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Quantizer(nn.Module):\n",
    "    def __init__(self, input_dim, G=2, V=320, temp_init=2.0, temp_min=0.5, temp_decay=0.999995):\n",
    "        super(Quantizer, self).__init__()\n",
    "        assert input_dim % G == 0, \"Input dimension must be divisible by number of groups G.\"\n",
    "        self.G = G  \n",
    "        self.V = V  \n",
    "        self.input_dim = input_dim\n",
    "        self.group_dim = input_dim // G\n",
    "        self.codebooks = nn.Parameter(torch.randn(G, V, input_dim // G))\n",
    "        self.proj = nn.Linear(input_dim, G * V)\n",
    "        self.register_buffer(\"_temperature\", torch.tensor(float(temp_init)), persistent=True)\n",
    "        self.temp_min = float(temp_min)\n",
    "        self.temp_decay = float(temp_decay)\n",
    "\n",
    "    @property\n",
    "    def temperature(self):\n",
    "        return float(self._temperature.item())\n",
    "\n",
    "    def update_temperature(self):\n",
    "        new_temp = max(self.temp_min, self.temperature * self.temp_decay)\n",
    "        self._temperature.fill_(new_temp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(\"Expected shape: [B, T, C]\")\n",
    "        \n",
    "        logits = self.proj(x)                              # [B, T, G * V]\n",
    "        B, T, _ = logits.shape\n",
    "        logits = logits.view(B, T, self.G, self.V)         # [B, T, G, V]\n",
    "        probs = F.gumbel_softmax(logits=logits, tau=self.temperature, dim=-1, hard=True)\n",
    "        cb = self.codebooks.unsqueeze(0).unsqueeze(0)    # [1, 1, G, V, C/G]\n",
    "        probs_expanded = probs.unsqueeze(-1)                  # [B, T, G, V, 1]\n",
    "        quantized = torch.sum(probs_expanded * cb, dim=-2)   # [B, T, G, C/G]\n",
    "        quantized = quantized.view(B, T, self.input_dim)      # [B, T, C]\n",
    "\n",
    "        soft = torch.softmax(logits, dim=-1)\n",
    "        p_bar = soft.mean(dim=(0,1))  \n",
    "        eps = 1e-9\n",
    "        diversity_loss = (p_bar * torch.log(p_bar + eps)).sum() / (self.G * self.V)\n",
    "        self.update_temperature()\n",
    "        return quantized, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99dfb3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.826387Z",
     "iopub.status.busy": "2025-12-07T17:58:08.826199Z",
     "iopub.status.idle": "2025-12-07T17:58:08.842597Z",
     "shell.execute_reply": "2025-12-07T17:58:08.841746Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.826372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout=.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=dropout,\n",
    "                                                    batch_first=True)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(nn.Linear(embed_dim, ffn_dim),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(ffn_dim, embed_dim),\n",
    "                                 nn.Dropout(dropout))\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.self_attention(x, x, x, key_padding_mask=padding_mask)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1f3785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.843564Z",
     "iopub.status.busy": "2025-12-07T17:58:08.843333Z",
     "iopub.status.idle": "2025-12-07T17:58:08.860705Z",
     "shell.execute_reply": "2025-12-07T17:58:08.860190Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.843543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, max_len, embed_dim) * 0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        return x + self.pos_emb[:, :T, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d238e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.861686Z",
     "iopub.status.busy": "2025-12-07T17:58:08.861443Z",
     "iopub.status.idle": "2025-12-07T17:58:08.874214Z",
     "shell.execute_reply": "2025-12-07T17:58:08.873658Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.861663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_layers=12, num_heads=8, \n",
    "                 ffn_dim=2048, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(embed_dim, num_heads, ffn_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        x = self.pos_embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, padding_mask)\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7d7f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.875327Z",
     "iopub.status.busy": "2025-12-07T17:58:08.875059Z",
     "iopub.status.idle": "2025-12-07T17:58:08.893817Z",
     "shell.execute_reply": "2025-12-07T17:58:08.893108Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.875311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Wav2Vec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 conv_channels=[512, 512, 512, 512, 512, 512, 512],\n",
    "                 kernel_sizes=[10, 3, 3, 3, 3, 2, 2],\n",
    "                 strides=[5, 2, 2, 2, 2, 2, 2],\n",
    "                 encoder_dropout=0.1,\n",
    "                 mask_prob=0.065,\n",
    "                 mask_length=10,\n",
    "                 embed_dim=512,\n",
    "                 num_transformer_layers=12,\n",
    "                 num_heads=8,\n",
    "                 ffn_dim=2048,\n",
    "                 transformer_dropout=0.1,\n",
    "                 num_groups=2,\n",
    "                 num_vars=320,\n",
    "                 temp_init=2.0,\n",
    "                 temp_min=0.5,\n",
    "                 temp_decay=0.999995,\n",
    "                 sample_rate=16000,\n",
    "                 seed=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            seed=seed,\n",
    "            in_channels=in_channels,\n",
    "            conv_channels=conv_channels,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            strides=strides,\n",
    "            dropout=encoder_dropout,\n",
    "            sample_rate=sample_rate\n",
    "        )\n",
    "        \n",
    "        self.masker = Masker(\n",
    "            embed_dim=embed_dim,\n",
    "            mask_prob=mask_prob,\n",
    "            mask_length=mask_length\n",
    "        )\n",
    "        \n",
    "        self.transformer = TransformerEncoder(\n",
    "            embed_dim=embed_dim,\n",
    "            num_layers=num_transformer_layers,\n",
    "            num_heads=num_heads,\n",
    "            ffn_dim=ffn_dim,\n",
    "            dropout=transformer_dropout,\n",
    "            max_len=5000\n",
    "        )\n",
    "        \n",
    "        self.quantizer = Quantizer(\n",
    "            input_dim=embed_dim,\n",
    "            G=num_groups,\n",
    "            V=num_vars,\n",
    "            temp_init=temp_init,\n",
    "            temp_min=temp_min,\n",
    "            temp_decay=temp_decay\n",
    "        )\n",
    "        \n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, audio, input_lengths):\n",
    "        features, output_lengths, padding_mask = self.encoder(audio, input_lengths)\n",
    "        \n",
    "        masked_features, mask_indices_b, mask_indices_t, mask = self.masker(features)\n",
    "        \n",
    "        contextualized = self.transformer(masked_features, padding_mask)\n",
    "        \n",
    "        quantized, diversity_loss = self.quantizer(features)\n",
    "        \n",
    "        return {\n",
    "            'contextualized': contextualized,\n",
    "            'quantized': quantized,\n",
    "            'mask_indices_b': mask_indices_b,\n",
    "            'mask_indices_t': mask_indices_t,\n",
    "            'diversity_loss': diversity_loss,\n",
    "            'features': features,\n",
    "            'padding_mask': padding_mask,\n",
    "            'output_lengths': output_lengths\n",
    "        }\n",
    "    \n",
    "    def compute_contrastive_loss(self, outputs, num_negatives=100, temperature=0.1):\n",
    "        contextualized = outputs['contextualized']\n",
    "        quantized = outputs['quantized']\n",
    "        mask_indices_b = outputs['mask_indices_b']\n",
    "        mask_indices_t = outputs['mask_indices_t']\n",
    "        \n",
    "        if len(mask_indices_b) == 0:\n",
    "            return torch.tensor(0.0, device=contextualized.device)\n",
    "        \n",
    "        c_masked = contextualized[mask_indices_b, mask_indices_t]  \n",
    "        q_masked = quantized[mask_indices_b, mask_indices_t]       \n",
    "        c_masked = self.projection(c_masked)  \n",
    "        \n",
    "        pos_logits = F.cosine_similarity(c_masked, q_masked, dim=-1)  \n",
    "        pos_logits = pos_logits / temperature\n",
    "        \n",
    "        B, T, C = quantized.shape\n",
    "        M = c_masked.shape[0]\n",
    "        \n",
    "        neg_indices = torch.randint(0, B * T, (M, num_negatives), device=quantized.device)\n",
    "        quantized_flat = quantized.view(-1, C)\n",
    "        negatives = quantized_flat[neg_indices] \n",
    "        \n",
    "        c_expanded = c_masked.unsqueeze(1)  \n",
    "        neg_logits = F.cosine_similarity(c_expanded, negatives, dim=-1)  \n",
    "        neg_logits = neg_logits / temperature\n",
    "        \n",
    "        logits = torch.cat([pos_logits.unsqueeze(1), neg_logits], dim=1)  \n",
    "        labels = torch.zeros(M, dtype=torch.long, device=logits.device)  \n",
    "        \n",
    "        contrastive_loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return contrastive_loss\n",
    "    \n",
    "    def compute_loss(self, outputs, num_negatives=100, temperature=0.1, diversity_weight=0.1):\n",
    "        contrastive_loss = self.compute_contrastive_loss(outputs, num_negatives, temperature)\n",
    "        diversity_loss = outputs['diversity_loss']\n",
    "        \n",
    "        total_loss = contrastive_loss + diversity_weight * diversity_loss\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss.mean().item() if total_loss.numel() > 1 else total_loss.item(),\n",
    "            'contrastive_loss': contrastive_loss.mean().item() if contrastive_loss.numel() > 1 else contrastive_loss.item(),\n",
    "            'diversity_loss': diversity_loss.mean().item() if diversity_loss.numel() > 1 else diversity_loss.item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664e41f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.894755Z",
     "iopub.status.busy": "2025-12-07T17:58:08.894534Z",
     "iopub.status.idle": "2025-12-07T17:58:08.912300Z",
     "shell.execute_reply": "2025-12-07T17:58:08.911569Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.894737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, epoch, num_negatives=100, temperature=0.1, diversity_weight=0.1):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_contrastive = 0.0\n",
    "    total_diversity = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        audio = batch['audio'].to(device)\n",
    "        lengths = batch['lengths'].to(device)\n",
    "        \n",
    "        outputs = model(audio, lengths)\n",
    "\n",
    "        model_ref = model.module if isinstance(model, nn.DataParallel) else model\n",
    "        loss, loss_dict = model_ref.compute_loss(\n",
    "            outputs,\n",
    "            num_negatives=num_negatives,\n",
    "            temperature=temperature,\n",
    "            diversity_weight=diversity_weight\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if loss.numel() > 1:\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss_dict['total_loss']\n",
    "        total_contrastive += loss_dict['contrastive_loss']\n",
    "        total_diversity += loss_dict['diversity_loss']\n",
    "        num_batches += 1\n",
    "        \n",
    "        if (batch_idx + 1) % 250 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epoch {epoch} | Batch {batch_idx + 1}/{len(dataloader)} | \"\n",
    "                  f\"Loss: {loss_dict['total_loss']:.4f} | \"\n",
    "                  f\"Contrastive: {loss_dict['contrastive_loss']:.4f} | \"\n",
    "                  f\"Diversity: {loss_dict['diversity_loss']:.4f} | \"\n",
    "                  f\"Time: {elapsed:.2f}s\")\n",
    "            start_time = time.time()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_contrastive = total_contrastive / num_batches\n",
    "    avg_diversity = total_diversity / num_batches\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'contrastive_loss': avg_contrastive,\n",
    "        'diversity_loss': avg_diversity\n",
    "    }\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, save_dir, filename=None):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"wav2vec_epoch{epoch}_{timestamp}.pt\"\n",
    "    \n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    \n",
    "    model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_state,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc97c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:08.913133Z",
     "iopub.status.busy": "2025-12-07T17:58:08.912867Z",
     "iopub.status.idle": "2025-12-07T17:58:09.010471Z",
     "shell.execute_reply": "2025-12-07T17:58:09.009620Z",
     "shell.execute_reply.started": "2025-12-07T17:58:08.913114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Device: cuda\n",
      "Data directory: /kaggle/input/libri100h/dev-clean\n",
      "Save directory: /kaggle/working/models\n",
      "Epochs: 20\n",
      "Batch size: 2\n",
      "Learning rate: 0.0005\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/kaggle/input/libri100h/dev-clean'\n",
    "SAVE_DIR = '/kaggle/working/models'\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 5e-4\n",
    "NUM_NEGATIVES = 100\n",
    "TEMPERATURE = 0.1\n",
    "DIVERSITY_WEIGHT = 0.1\n",
    "SAVE_EVERY = 5\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Save directory: {SAVE_DIR}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e52cbb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:09.011612Z",
     "iopub.status.busy": "2025-12-07T17:58:09.011257Z",
     "iopub.status.idle": "2025-12-07T17:58:10.345251Z",
     "shell.execute_reply": "2025-12-07T17:58:10.344576Z",
     "shell.execute_reply.started": "2025-12-07T17:58:09.011589Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset\n",
      "Dataset size: 2703 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading dataset\")\n",
    "dataset = LibriSpeechDataset(DATA_DIR)\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "\n",
    "num_workers = 4 if DEVICE == 'cuda' else 0\n",
    "pin_memory = DEVICE == 'cuda'\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=True if num_workers > 0 else False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91567f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:10.346510Z",
     "iopub.status.busy": "2025-12-07T17:58:10.346091Z",
     "iopub.status.idle": "2025-12-07T17:58:13.711940Z",
     "shell.execute_reply": "2025-12-07T17:58:13.711408Z",
     "shell.execute_reply.started": "2025-12-07T17:58:10.346492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model\n",
      "Model parameters: 45,351,552\n",
      "Using 2 GPUs with DataParallel\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing model\")\n",
    "model = Wav2Vec(\n",
    "    in_channels=1,\n",
    "    conv_channels=[512, 512, 512, 512, 512, 512, 512],\n",
    "    kernel_sizes=[10, 3, 3, 3, 3, 2, 2],\n",
    "    strides=[5, 2, 2, 2, 2, 2, 2],\n",
    "    encoder_dropout=0.1,\n",
    "    mask_prob=0.065,\n",
    "    mask_length=10,\n",
    "    embed_dim=512,\n",
    "    num_transformer_layers=12,\n",
    "    num_heads=8,\n",
    "    ffn_dim=2048,\n",
    "    transformer_dropout=0.1,\n",
    "    num_groups=2,\n",
    "    num_vars=320,\n",
    "    temp_init=2.0,\n",
    "    temp_min=0.5,\n",
    "    temp_decay=0.999995,\n",
    "    sample_rate=16000,\n",
    "    seed=42\n",
    ").to(DEVICE)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "if DEVICE == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "elif DEVICE == 'cuda':\n",
    "    print(f\"Using single GPU\")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "649ceb1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:58:13.712815Z",
     "iopub.status.busy": "2025-12-07T17:58:13.712570Z",
     "iopub.status.idle": "2025-12-07T19:28:06.017635Z",
     "shell.execute_reply": "2025-12-07T19:28:06.016501Z",
     "shell.execute_reply.started": "2025-12-07T17:58:13.712800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 250/1352 | Loss: 4.5250 | Contrastive: 4.5266 | Diversity: -0.0166 | Time: 50.63s\n",
      "Epoch 1 | Batch 500/1352 | Loss: 4.3062 | Contrastive: 4.3068 | Diversity: -0.0056 | Time: 47.04s\n",
      "Epoch 1 | Batch 750/1352 | Loss: 4.6041 | Contrastive: 4.6044 | Diversity: -0.0026 | Time: 47.25s\n",
      "Epoch 1 | Batch 1000/1352 | Loss: 4.6141 | Contrastive: 4.6145 | Diversity: -0.0036 | Time: 50.25s\n",
      "Epoch 1 | Batch 1250/1352 | Loss: 4.6236 | Contrastive: 4.6240 | Diversity: -0.0041 | Time: 50.47s\n",
      "Epoch 1 Summary:\n",
      "  Average Loss: 4.5892\n",
      "  Contrastive Loss: 4.5900\n",
      "  Diversity Loss: -0.0074\n",
      "EPOCH 2/20\n",
      "Epoch 2 | Batch 250/1352 | Loss: 4.6232 | Contrastive: 4.6237 | Diversity: -0.0051 | Time: 49.99s\n",
      "Epoch 2 | Batch 500/1352 | Loss: 4.6230 | Contrastive: 4.6235 | Diversity: -0.0055 | Time: 50.56s\n",
      "Epoch 2 | Batch 750/1352 | Loss: 4.6163 | Contrastive: 4.6168 | Diversity: -0.0053 | Time: 50.83s\n",
      "Epoch 2 | Batch 1000/1352 | Loss: 4.6133 | Contrastive: 4.6139 | Diversity: -0.0063 | Time: 47.87s\n",
      "Epoch 2 | Batch 1250/1352 | Loss: 4.6118 | Contrastive: 4.6124 | Diversity: -0.0058 | Time: 50.30s\n",
      "Epoch 2 Summary:\n",
      "  Average Loss: 4.6154\n",
      "  Contrastive Loss: 4.6159\n",
      "  Diversity Loss: -0.0054\n",
      "EPOCH 3/20\n",
      "Epoch 3 | Batch 250/1352 | Loss: 4.6152 | Contrastive: 4.6158 | Diversity: -0.0068 | Time: 50.42s\n",
      "Epoch 3 | Batch 500/1352 | Loss: 4.6114 | Contrastive: 4.6121 | Diversity: -0.0073 | Time: 47.62s\n",
      "Epoch 3 | Batch 750/1352 | Loss: 4.6186 | Contrastive: 4.6193 | Diversity: -0.0066 | Time: 50.26s\n",
      "Epoch 3 | Batch 1000/1352 | Loss: 4.6048 | Contrastive: 4.6052 | Diversity: -0.0043 | Time: 52.63s\n",
      "Epoch 3 | Batch 1250/1352 | Loss: 4.6184 | Contrastive: 4.6191 | Diversity: -0.0068 | Time: 49.77s\n",
      "Epoch 3 Summary:\n",
      "  Average Loss: 4.6159\n",
      "  Contrastive Loss: 4.6165\n",
      "  Diversity Loss: -0.0065\n",
      "EPOCH 4/20\n",
      "Epoch 4 | Batch 250/1352 | Loss: 4.6127 | Contrastive: 4.6131 | Diversity: -0.0041 | Time: 49.35s\n",
      "Epoch 4 | Batch 500/1352 | Loss: 4.6168 | Contrastive: 4.6172 | Diversity: -0.0046 | Time: 51.49s\n",
      "Epoch 4 | Batch 750/1352 | Loss: 4.6147 | Contrastive: 4.6153 | Diversity: -0.0056 | Time: 49.02s\n",
      "Epoch 4 | Batch 1000/1352 | Loss: 4.6228 | Contrastive: 4.6234 | Diversity: -0.0057 | Time: 48.97s\n",
      "Epoch 4 | Batch 1250/1352 | Loss: 4.6370 | Contrastive: 4.6377 | Diversity: -0.0066 | Time: 49.30s\n",
      "Epoch 4 Summary:\n",
      "  Average Loss: 4.6148\n",
      "  Contrastive Loss: 4.6154\n",
      "  Diversity Loss: -0.0056\n",
      "EPOCH 5/20\n",
      "Epoch 5 | Batch 250/1352 | Loss: 4.6340 | Contrastive: 4.6348 | Diversity: -0.0084 | Time: 48.83s\n",
      "Epoch 5 | Batch 500/1352 | Loss: 4.6327 | Contrastive: 4.6336 | Diversity: -0.0086 | Time: 47.38s\n",
      "Epoch 5 | Batch 1000/1352 | Loss: 4.6358 | Contrastive: 4.6367 | Diversity: -0.0091 | Time: 51.32s\n",
      "Epoch 5 | Batch 1250/1352 | Loss: 4.6270 | Contrastive: 4.6280 | Diversity: -0.0102 | Time: 50.44s\n",
      "Epoch 5 Summary:\n",
      "  Average Loss: 4.6161\n",
      "  Contrastive Loss: 4.6170\n",
      "  Diversity Loss: -0.0087\n",
      "EPOCH 6/20\n",
      "Epoch 6 | Batch 250/1352 | Loss: 4.6201 | Contrastive: 4.6210 | Diversity: -0.0092 | Time: 49.58s\n",
      "Epoch 6 | Batch 500/1352 | Loss: 4.5971 | Contrastive: 4.5981 | Diversity: -0.0091 | Time: 49.10s\n",
      "Epoch 6 | Batch 750/1352 | Loss: 4.6270 | Contrastive: 4.6279 | Diversity: -0.0093 | Time: 51.79s\n",
      "Epoch 6 | Batch 1000/1352 | Loss: 4.6174 | Contrastive: 4.6183 | Diversity: -0.0090 | Time: 51.20s\n",
      "Epoch 6 | Batch 1250/1352 | Loss: 4.6013 | Contrastive: 4.6022 | Diversity: -0.0090 | Time: 49.51s\n",
      "Epoch 6 Summary:\n",
      "  Average Loss: 4.6180\n",
      "  Contrastive Loss: 4.6189\n",
      "  Diversity Loss: -0.0090\n",
      "EPOCH 7/20\n",
      "Epoch 7 | Batch 250/1352 | Loss: 4.6189 | Contrastive: 4.6197 | Diversity: -0.0084 | Time: 49.29s\n",
      "Epoch 7 | Batch 500/1352 | Loss: 4.6305 | Contrastive: 4.6313 | Diversity: -0.0085 | Time: 50.20s\n",
      "Epoch 7 | Batch 750/1352 | Loss: 4.6188 | Contrastive: 4.6197 | Diversity: -0.0084 | Time: 51.24s\n",
      "Epoch 7 | Batch 1000/1352 | Loss: 4.6063 | Contrastive: 4.6071 | Diversity: -0.0081 | Time: 50.62s\n",
      "Epoch 7 | Batch 1250/1352 | Loss: 4.6009 | Contrastive: 4.6017 | Diversity: -0.0083 | Time: 49.35s\n",
      "Epoch 7 Summary:\n",
      "  Average Loss: 4.6164\n",
      "  Contrastive Loss: 4.6172\n",
      "  Diversity Loss: -0.0083\n",
      "EPOCH 8/20\n",
      "Epoch 8 | Batch 250/1352 | Loss: 4.6200 | Contrastive: 4.6208 | Diversity: -0.0080 | Time: 49.82s\n",
      "Epoch 8 | Batch 500/1352 | Loss: 4.6324 | Contrastive: 4.6332 | Diversity: -0.0082 | Time: 50.14s\n",
      "Epoch 8 | Batch 750/1352 | Loss: 4.6291 | Contrastive: 4.6299 | Diversity: -0.0083 | Time: 49.67s\n",
      "Epoch 8 | Batch 1000/1352 | Loss: 4.6156 | Contrastive: 4.6164 | Diversity: -0.0083 | Time: 49.91s\n",
      "Epoch 8 | Batch 1250/1352 | Loss: 4.6293 | Contrastive: 4.6301 | Diversity: -0.0080 | Time: 50.12s\n",
      "Epoch 8 Summary:\n",
      "  Average Loss: 4.6171\n",
      "  Contrastive Loss: 4.6179\n",
      "  Diversity Loss: -0.0081\n",
      "EPOCH 9/20\n",
      "Epoch 9 | Batch 250/1352 | Loss: 4.5977 | Contrastive: 4.5985 | Diversity: -0.0076 | Time: 47.89s\n",
      "Epoch 9 | Batch 500/1352 | Loss: 4.6254 | Contrastive: 4.6262 | Diversity: -0.0079 | Time: 50.01s\n",
      "Epoch 9 | Batch 750/1352 | Loss: 4.5607 | Contrastive: 4.5615 | Diversity: -0.0082 | Time: 50.06s\n",
      "Epoch 9 | Batch 1000/1352 | Loss: 4.6151 | Contrastive: 4.6159 | Diversity: -0.0088 | Time: 49.65s\n",
      "Epoch 9 | Batch 1250/1352 | Loss: 4.5856 | Contrastive: 4.5864 | Diversity: -0.0083 | Time: 51.99s\n",
      "Epoch 9 Summary:\n",
      "  Average Loss: 4.6171\n",
      "  Contrastive Loss: 4.6179\n",
      "  Diversity Loss: -0.0081\n",
      "EPOCH 10/20\n",
      "Epoch 10 | Batch 250/1352 | Loss: 4.6209 | Contrastive: 4.6218 | Diversity: -0.0084 | Time: 48.99s\n",
      "Epoch 10 | Batch 500/1352 | Loss: 4.6292 | Contrastive: 4.6299 | Diversity: -0.0073 | Time: 51.58s\n",
      "Epoch 10 | Batch 750/1352 | Loss: 4.6184 | Contrastive: 4.6192 | Diversity: -0.0078 | Time: 48.99s\n",
      "Epoch 10 | Batch 1000/1352 | Loss: 4.6271 | Contrastive: 4.6279 | Diversity: -0.0080 | Time: 49.83s\n",
      "Epoch 10 | Batch 1250/1352 | Loss: 4.6486 | Contrastive: 4.6494 | Diversity: -0.0084 | Time: 50.42s\n",
      "Epoch 11 | Batch 500/1352 | Loss: 4.6164 | Contrastive: 4.6170 | Diversity: -0.0061 | Time: 45.82s\n",
      "Epoch 11 | Batch 1250/1352 | Loss: 4.6122 | Contrastive: 4.6128 | Diversity: -0.0061 | Time: 53.50s\n",
      "Epoch 11 Summary:\n",
      "  Average Loss: 4.6155\n",
      "  Contrastive Loss: 4.6162\n",
      "  Diversity Loss: -0.0069\n",
      "EPOCH 12/20\n",
      "Epoch 12 | Batch 250/1352 | Loss: 4.6087 | Contrastive: 4.6094 | Diversity: -0.0070 | Time: 50.90s\n",
      "Epoch 12 | Batch 500/1352 | Loss: 4.6120 | Contrastive: 4.6128 | Diversity: -0.0077 | Time: 50.52s\n",
      "Epoch 12 | Batch 750/1352 | Loss: 4.5989 | Contrastive: 4.5997 | Diversity: -0.0074 | Time: 49.37s\n",
      "Epoch 12 | Batch 1250/1352 | Loss: 4.5989 | Contrastive: 4.5997 | Diversity: -0.0084 | Time: 49.57s\n",
      "Epoch 12 Summary:\n",
      "  Average Loss: 4.6155\n",
      "  Contrastive Loss: 4.6163\n",
      "  Diversity Loss: -0.0075\n",
      "EPOCH 13/20\n",
      "Epoch 13 | Batch 250/1352 | Loss: 4.6498 | Contrastive: 4.6507 | Diversity: -0.0088 | Time: 49.16s\n",
      "Epoch 13 | Batch 500/1352 | Loss: 4.6061 | Contrastive: 4.6070 | Diversity: -0.0084 | Time: 50.84s\n",
      "Epoch 13 | Batch 750/1352 | Loss: 4.6280 | Contrastive: 4.6289 | Diversity: -0.0084 | Time: 47.63s\n",
      "Epoch 13 Summary:\n",
      "  Average Loss: 4.6173\n",
      "  Contrastive Loss: 4.6182\n",
      "  Diversity Loss: -0.0087\n",
      "EPOCH 14/20\n",
      "Epoch 14 | Batch 250/1352 | Loss: 4.6050 | Contrastive: 4.6059 | Diversity: -0.0088 | Time: 49.69s\n",
      "Epoch 14 | Batch 500/1352 | Loss: 4.6073 | Contrastive: 4.6081 | Diversity: -0.0085 | Time: 50.45s\n",
      "Epoch 14 | Batch 750/1352 | Loss: 4.6169 | Contrastive: 4.6177 | Diversity: -0.0077 | Time: 50.31s\n",
      "Epoch 14 | Batch 1000/1352 | Loss: 4.6163 | Contrastive: 4.6171 | Diversity: -0.0074 | Time: 50.32s\n",
      "Epoch 14 | Batch 1250/1352 | Loss: 4.6159 | Contrastive: 4.6167 | Diversity: -0.0074 | Time: 48.95s\n",
      "Epoch 14 Summary:\n",
      "  Average Loss: 4.6184\n",
      "  Contrastive Loss: 4.6192\n",
      "  Diversity Loss: -0.0082\n",
      "EPOCH 15/20\n",
      "Epoch 15 | Batch 250/1352 | Loss: 4.6087 | Contrastive: 4.6095 | Diversity: -0.0073 | Time: 47.98s\n",
      "Epoch 15 | Batch 500/1352 | Loss: 4.6066 | Contrastive: 4.6073 | Diversity: -0.0075 | Time: 50.40s\n",
      "Epoch 15 | Batch 750/1352 | Loss: 4.6195 | Contrastive: 4.6202 | Diversity: -0.0078 | Time: 49.51s\n",
      "Epoch 15 | Batch 1000/1352 | Loss: 4.6024 | Contrastive: 4.6032 | Diversity: -0.0078 | Time: 53.93s\n",
      "Epoch 15 | Batch 1250/1352 | Loss: 4.6171 | Contrastive: 4.6178 | Diversity: -0.0076 | Time: 47.58s\n",
      "Epoch 15 Summary:\n",
      "  Average Loss: 4.6160\n",
      "  Contrastive Loss: 4.6168\n",
      "  Diversity Loss: -0.0075\n",
      "EPOCH 16/20\n",
      "Epoch 16 | Batch 250/1352 | Loss: 4.6154 | Contrastive: 4.6161 | Diversity: -0.0076 | Time: 47.78s\n",
      "Epoch 16 | Batch 500/1352 | Loss: 4.6232 | Contrastive: 4.6240 | Diversity: -0.0084 | Time: 50.02s\n",
      "Epoch 16 | Batch 750/1352 | Loss: 4.6132 | Contrastive: 4.6140 | Diversity: -0.0082 | Time: 50.45s\n",
      "Epoch 16 | Batch 1000/1352 | Loss: 4.6289 | Contrastive: 4.6296 | Diversity: -0.0077 | Time: 49.97s\n",
      "Epoch 17 | Batch 250/1352 | Loss: 4.6271 | Contrastive: 4.6279 | Diversity: -0.0079 | Time: 49.81s\n",
      "Epoch 17 | Batch 500/1352 | Loss: 4.6192 | Contrastive: 4.6199 | Diversity: -0.0073 | Time: 51.67s\n",
      "Epoch 17 | Batch 750/1352 | Loss: 4.6270 | Contrastive: 4.6278 | Diversity: -0.0070 | Time: 51.20s\n",
      "Epoch 17 | Batch 1000/1352 | Loss: 4.6302 | Contrastive: 4.6309 | Diversity: -0.0070 | Time: 47.15s\n",
      "Epoch 17 | Batch 1250/1352 | Loss: 4.6284 | Contrastive: 4.6291 | Diversity: -0.0071 | Time: 49.23s\n",
      "Epoch 17 Summary:\n",
      "  Average Loss: 4.6160\n",
      "  Contrastive Loss: 4.6167\n",
      "  Diversity Loss: -0.0073\n",
      "EPOCH 18/20\n",
      "Epoch 18 | Batch 250/1352 | Loss: 4.6001 | Contrastive: 4.6009 | Diversity: -0.0073 | Time: 49.38s\n",
      "Epoch 18 | Batch 500/1352 | Loss: 4.6103 | Contrastive: 4.6111 | Diversity: -0.0075 | Time: 51.19s\n",
      "Epoch 18 | Batch 750/1352 | Loss: 4.6083 | Contrastive: 4.6090 | Diversity: -0.0071 | Time: 48.48s\n",
      "Epoch 18 | Batch 1000/1352 | Loss: 4.6229 | Contrastive: 4.6236 | Diversity: -0.0073 | Time: 50.64s\n",
      "Epoch 18 | Batch 1250/1352 | Loss: 4.6076 | Contrastive: 4.6083 | Diversity: -0.0069 | Time: 48.97s\n",
      "Epoch 18 Summary:\n",
      "  Average Loss: 4.6161\n",
      "  Contrastive Loss: 4.6168\n",
      "  Diversity Loss: -0.0072\n",
      "EPOCH 19/20\n",
      "Epoch 19 | Batch 250/1352 | Loss: 4.6141 | Contrastive: 4.6148 | Diversity: -0.0068 | Time: 49.37s\n",
      "Epoch 19 | Batch 500/1352 | Loss: 4.6058 | Contrastive: 4.6065 | Diversity: -0.0067 | Time: 51.34s\n",
      "Epoch 19 | Batch 750/1352 | Loss: 4.6078 | Contrastive: 4.6085 | Diversity: -0.0069 | Time: 50.94s\n",
      "Epoch 19 | Batch 1000/1352 | Loss: 4.6157 | Contrastive: 4.6164 | Diversity: -0.0068 | Time: 49.79s\n",
      "Epoch 19 | Batch 1250/1352 | Loss: 4.6245 | Contrastive: 4.6252 | Diversity: -0.0063 | Time: 47.28s\n",
      "Epoch 19 Summary:\n",
      "  Average Loss: 4.6155\n",
      "  Contrastive Loss: 4.6162\n",
      "  Diversity Loss: -0.0068\n",
      "EPOCH 20/20\n",
      "Epoch 20 | Batch 250/1352 | Loss: 4.6044 | Contrastive: 4.6050 | Diversity: -0.0063 | Time: 49.81s\n",
      "Epoch 20 | Batch 500/1352 | Loss: 4.6126 | Contrastive: 4.6132 | Diversity: -0.0062 | Time: 48.29s\n",
      "Epoch 20 | Batch 750/1352 | Loss: 4.5953 | Contrastive: 4.5959 | Diversity: -0.0057 | Time: 48.65s\n",
      "Epoch 20 | Batch 1000/1352 | Loss: 4.6313 | Contrastive: 4.6319 | Diversity: -0.0062 | Time: 52.51s\n",
      "Epoch 20 | Batch 1250/1352 | Loss: 4.6231 | Contrastive: 4.6236 | Diversity: -0.0058 | Time: 49.07s\n",
      "Epoch 20 Summary:\n",
      "  Average Loss: 4.6148\n",
      "  Contrastive Loss: 4.6154\n",
      "  Diversity Loss: -0.0062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/models/final_model.pt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"EPOCH {epoch}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    metrics = train_epoch(\n",
    "        model, dataloader, optimizer, DEVICE, epoch,\n",
    "        num_negatives=NUM_NEGATIVES,\n",
    "        temperature=TEMPERATURE,\n",
    "        diversity_weight=DIVERSITY_WEIGHT\n",
    "    )\n",
    "    \n",
    "    print(f\"Epoch {epoch} Summary:\")\n",
    "    print(f\"  Average Loss: {metrics['loss']:.4f}\")\n",
    "    print(f\"  Contrastive Loss: {metrics['contrastive_loss']:.4f}\")\n",
    "    print(f\"  Diversity Loss: {metrics['diversity_loss']:.4f}\")\n",
    "    \n",
    "    if epoch % SAVE_EVERY == 0:\n",
    "        save_checkpoint(model, optimizer, epoch, metrics['loss'], SAVE_DIR)\n",
    "    \n",
    "    if metrics['loss'] < best_loss:\n",
    "        best_loss = metrics['loss']\n",
    "        save_checkpoint(model, optimizer, epoch, metrics['loss'], SAVE_DIR, \n",
    "                      filename='best_model.pt')\n",
    "\n",
    "save_checkpoint(model, optimizer, NUM_EPOCHS, metrics['loss'], SAVE_DIR, \n",
    "               filename='final_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8942537,
     "sourceId": 14046714,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8942501,
     "sourceId": 14046663,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
